{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP text classification for corona virus tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "park= SparkSession.builder.appName('nlp').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import MinMaxScaler, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "#ML Logisitic Regression\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset= spark.read.csv('Corona_NLP_train.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(UserName='3799', ScreenName='48751', Location='London', TweetAt='16-03-2020', OriginalTweet='@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/iFz9FAn2Pa and https://t.co/xX6ghGFzCC and https://t.co/I2NlzdxNo8', Sentiment='Neutral')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+---------+\n",
      "|            UserName|          ScreenName|            Location|             TweetAt|       OriginalTweet|Sentiment|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+---------+\n",
      "|                3799|               48751|              London|          16-03-2020|@MeNyrbie @Phil_G...|  Neutral|\n",
      "|                3800|               48752|                  UK|          16-03-2020|advice Talk to yo...| Positive|\n",
      "|                3801|               48753|           Vagabonds|          16-03-2020|Coronavirus Austr...| Positive|\n",
      "|                3802|               48754|                null|          16-03-2020|My food stock is ...|     null|\n",
      "|              PLEASE|         don't panic| THERE WILL BE EN...|                null|                null|     null|\n",
      "|           Stay calm|          stay safe.|                null|                null|                null|     null|\n",
      "|#COVID19france #C...|            Positive|                null|                null|                null|     null|\n",
      "|                3803|               48755|                null|          16-03-2020|Me, ready to go a...|     null|\n",
      "|Not because I'm p...| but because my f...|          but please| don't panic. It ...|                null|     null|\n",
      "|#CoronavirusFranc...|  Extremely Negative|                null|                null|                null|     null|\n",
      "|                3804|               48756|ÜT: 36.319708,-82...|          16-03-2020|As news of the re...| Positive|\n",
      "|                3805|               48757|35.926541,-78.753267|          16-03-2020|\"Cashier at groce...| Positive|\n",
      "|                3806|               48758|             Austria|          16-03-2020|Was at the superm...|     null|\n",
      "|#toiletpapercrisi...|             Neutral|                null|                null|                null|     null|\n",
      "|                3807|               48759|     Atlanta, GA USA|          16-03-2020|Due to COVID-19 o...| Positive|\n",
      "|                3808|               48760|    BHAVNAGAR,GUJRAT|          16-03-2020|For corona preven...| Negative|\n",
      "|                3809|               48761|      Makati, Manila|          16-03-2020|All month there h...|  Neutral|\n",
      "|                3810|               48762|Pitt Meadows, BC,...|          16-03-2020|Due to the Covid-...|     null|\n",
      "|The wait time may...| particularly bee...|                null|                null|                null|     null|\n",
      "|We thank you for ...|  Extremely Positive|                null|                null|                null|     null|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|       OriginalTweet|Sentiment|\n",
      "+--------------------+---------+\n",
      "|@MeNyrbie @Phil_G...|  Neutral|\n",
      "|advice Talk to yo...| Positive|\n",
      "|Coronavirus Austr...| Positive|\n",
      "|My food stock is ...|     null|\n",
      "|                null|     null|\n",
      "+--------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.select(\"OriginalTweet\", \"Sentiment\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list= ['UserName', 'ScreenName', 'Location', 'TweetAt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset= dataset.select([column for column in dataset.columns if column not in drop_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|       OriginalTweet|Sentiment|\n",
      "+--------------------+---------+\n",
      "|@MeNyrbie @Phil_G...|  Neutral|\n",
      "|advice Talk to yo...| Positive|\n",
      "|Coronavirus Austr...| Positive|\n",
      "|My food stock is ...|     null|\n",
      "|                null|     null|\n",
      "|                null|     null|\n",
      "|                null|     null|\n",
      "|Me, ready to go a...|     null|\n",
      "|                null|     null|\n",
      "|                null|     null|\n",
      "|As news of the re...| Positive|\n",
      "|\"Cashier at groce...| Positive|\n",
      "|Was at the superm...|     null|\n",
      "|                null|     null|\n",
      "|Due to COVID-19 o...| Positive|\n",
      "|For corona preven...| Negative|\n",
      "|All month there h...|  Neutral|\n",
      "|Due to the Covid-...|     null|\n",
      "|                null|     null|\n",
      "|                null|     null|\n",
      "+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28617"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.dropna()\n",
    "dataset.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28617"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.dropna()\n",
    "dataset.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|           Sentiment|count|\n",
      "+--------------------+-----+\n",
      "|            Positive| 7718|\n",
      "|            Negative| 6857|\n",
      "|             Neutral| 5224|\n",
      "|  Extremely Positive| 4412|\n",
      "|  Extremely Negative| 3751|\n",
      "|   social distancing|    5|\n",
      "|    N. Y. - April 10|    3|\n",
      "|        Corona Virus|    2|\n",
      "|        Stay with us|    2|\n",
      "| supermarket workers|    2|\n",
      "|           of course|    2|\n",
      "| but we also need...|    2|\n",
      "| or click the lin...|    2|\n",
      "|            delivery|    2|\n",
      "| state governors ...|    2|\n",
      "| not going to the...|    2|\n",
      "| ecological collapse|    2|\n",
      "|             however|    2|\n",
      "| just \"\"selfish p...|    2|\n",
      "| as lower oil pri...|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Checking the type of sentiments\n",
    "from pyspark.sql.functions import col\n",
    "dataset.groupBy(\"Sentiment\").count().orderBy(col(\"count\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+\n",
      "|         Sentiment|count|\n",
      "+------------------+-----+\n",
      "|          Positive| 7718|\n",
      "|          Negative| 6857|\n",
      "|           Neutral| 5224|\n",
      "|Extremely Positive| 4412|\n",
      "|Extremely Negative| 3751|\n",
      "+------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Filtering the data to get Sentiment in terms of Positive, Negative,Neutral,Extremely Positive and Extremely Negative values\n",
    "import pyspark.sql.functions as fn\n",
    "data = dataset.where(fn.col(\"Sentiment\").isin([\"Positive\", \"Negative\", \"Neutral\",\"Extremely Positive\",\"Extremely Negative\"]))\n",
    "data.groupBy(\"Sentiment\").count().orderBy(col(\"count\").desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizers, stopwords has been used to transform the data to find the count vector of the feature column for the  modelling purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "# using tokenizer\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"OriginalTweet\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "# checking the stop words\n",
    "add_stopwords = [\"http\",\"https\",\"amp\",\"rt\",\"t\",\"c\",\"the\"] \n",
    "stopwordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\").setStopWords(add_stopwords)\n",
    "# checking the bag of words count\n",
    "countVectors = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", vocabSize=1000, minDF=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert the text to label, String indexing has been used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+--------------------+--------------------+--------------------+-----+\n",
      "|       OriginalTweet|         Sentiment|               words|            filtered|            features|label|\n",
      "+--------------------+------------------+--------------------+--------------------+--------------------+-----+\n",
      "|@MeNyrbie @Phil_G...|           Neutral|[menyrbie, phil_g...|[menyrbie, phil_g...|(1000,[1,5],[2.0,...|  2.0|\n",
      "|advice Talk to yo...|          Positive|[advice, talk, to...|[advice, talk, to...|(1000,[0,2,25,34,...|  0.0|\n",
      "|Coronavirus Austr...|          Positive|[coronavirus, aus...|[coronavirus, aus...|(1000,[0,5,6,8,9,...|  0.0|\n",
      "|As news of the re...|          Positive|[as, news, of, th...|[as, news, of, re...|(1000,[0,1,2,5,8,...|  0.0|\n",
      "|\"Cashier at groce...|          Positive|[cashier, at, gro...|[cashier, at, gro...|(1000,[0,4,5,11,1...|  0.0|\n",
      "|Due to COVID-19 o...|          Positive|[due, to, covid, ...|[due, to, covid, ...|(1000,[0,1,4,5,7,...|  0.0|\n",
      "|For corona preven...|          Negative|[for, corona, pre...|[for, corona, pre...|(1000,[0,1,7,8,9,...|  1.0|\n",
      "|All month there h...|           Neutral|[all, month, ther...|[all, month, ther...|(1000,[1,3,4,5,10...|  2.0|\n",
      "|#horningsea is a ...|Extremely Positive|[horningsea, is, ...|[horningsea, is, ...|(1000,[0,1,3,4,5,...|  3.0|\n",
      "|ADARA Releases CO...|          Positive|[adara, releases,...|[adara, releases,...|(1000,[0,5,7,8,9,...|  0.0|\n",
      "+--------------------+------------------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "label_stringIdx = StringIndexer(inputCol = \"Sentiment\", outputCol = \"label\")\n",
    "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors, label_stringIdx])\n",
    "pipelineFit = pipeline.fit(data)\n",
    "dataset = pipelineFit.transform(data)\n",
    "dataset.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the dataset into train data and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Count: 21023\n",
      "Test Data Count: 6939\n"
     ]
    }
   ],
   "source": [
    "(trainData, testData) = dataset.randomSplit([0.75, 0.25], seed=120)\n",
    "print(\"Training Data Count: \" + str(trainData.count()))\n",
    "print(\"Test Data Count: \" + str(testData.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+------------------+------------------------------+-----+----------+\n",
      "|                 OriginalTweet|         Sentiment|                   probability|label|prediction|\n",
      "+------------------------------+------------------+------------------------------+-----+----------+\n",
      "|Everyone stay safe and be c...|Extremely Positive|[0.31715463502639174,0.2069...|  3.0|       0.0|\n",
      "|The craft-distilling indust...|Extremely Positive|[0.3170706909168704,0.21139...|  3.0|       0.0|\n",
      "|We would like to thank @Bar...|Extremely Positive|[0.31471189504562613,0.1862...|  3.0|       0.0|\n",
      "|Wash your hands frequently ...|Extremely Positive|[0.31271377825392316,0.2072...|  3.0|       0.0|\n",
      "|Our thanks to a longtime pa...|Extremely Positive|[0.3125189388505169,0.20390...|  3.0|       0.0|\n",
      "|Its pathetic that it feels...|Extremely Positive|[0.3113314710153119,0.20507...|  3.0|       0.0|\n",
      "|Want a sweet happy feel goo...|Extremely Positive|[0.311301786878458,0.203847...|  3.0|       0.0|\n",
      "|Good and Bad DIY Hand Sanit...|          Positive|[0.3109144478626468,0.19648...|  0.0|       0.0|\n",
      "|@MamataOfficial @WBPolice #...|Extremely Positive|[0.31043908299611556,0.1971...|  3.0|       0.0|\n",
      "|In addition to our hand sa...|Extremely Positive|[0.31040828499642703,0.2027...|  3.0|       0.0|\n",
      "+------------------------------+------------------+------------------------------+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(labelCol=\"label\", \n",
    "                            featuresCol=\"features\",\n",
    "                            numTrees = 100, \n",
    "                            maxDepth = 4, \n",
    "                            maxBins = 32)\n",
    "# Training the model\n",
    "rfcModel = rfc.fit(trainData)\n",
    "# Making prediction\n",
    "predictions = rfcModel.transform(testData)\n",
    "predictions.filter(predictions['prediction'] == 0) .select(\"OriginalTweet\",\"Sentiment\",\"probability\",\"label\",\"prediction\") .orderBy(\"probability\", ascending=False)  .show(n = 10, truncate = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15665602788233612"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Accuracy of model\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy for Random Forest Classifier model is 15.66%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+------------------+------------------------------+-----+----------+\n",
      "|                 OriginalTweet|         Sentiment|                   probability|label|prediction|\n",
      "+------------------------------+------------------+------------------------------+-----+----------+\n",
      "|@berndsankara Hi. We're wor...|          Positive|[0.6257999141116156,0.15840...|  0.0|       0.0|\n",
      "|@adrparsons @commerson @Mic...|          Positive|[0.5991311183043058,0.12418...|  0.0|       0.0|\n",
      "|As we take stock of the pos...|Extremely Positive|[0.595381293274773,0.125248...|  3.0|       0.0|\n",
      "|The #coronavirus must act l...|Extremely Positive|[0.592823084833878,0.101718...|  3.0|       0.0|\n",
      "|@wearyrabbit Hi there - We'...|          Positive|[0.5926720150112601,0.18631...|  0.0|       0.0|\n",
      "|Empty store shelves hurt pe...|          Negative|[0.5898156981859813,0.12496...|  1.0|       0.0|\n",
      "|I told my Mom &amp; Dad tha...|          Positive|[0.5822067121323986,0.07961...|  0.0|       0.0|\n",
      "|The currently recommends co...|Extremely Positive|[0.5703540022009753,0.12998...|  3.0|       0.0|\n",
      "|To summarize Ducey s Action...|Extremely Positive|[0.5686081288281261,0.11212...|  3.0|       0.0|\n",
      "|Remember back in the day wh...|          Positive|[0.5659371807518201,0.14822...|  0.0|       0.0|\n",
      "+------------------------------+------------------+------------------------------+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "logrModel = logr.fit(trainData)\n",
    "# making prediction on test data \n",
    "predictions = logrModel.transform(testData)\n",
    "predictions.filter(predictions['prediction'] == 0) .select(\"OriginalTweet\",\"Sentiment\",\"probability\",\"label\",\"prediction\") .orderBy(\"probability\", ascending=False) .show(n = 10, truncate = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48468548688085306"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Accuracy of model\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our anaysis, we can see that Logistic Regression model gives the accuracy 48.46% for the dataset however, Random forest classifier model gives an accuracy of 15.66%.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
